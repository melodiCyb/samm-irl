import argparse
import numpy as np
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

from train_airl import ResetCompat, StepCompat, FloatObs, to_float32

def make_eval_env(env_id: str, seed: int):
    env = gym.make(env_id, disable_env_checker=True)
    env.reset(seed=seed)
    env.action_space.seed(seed)
    env.observation_space.seed(seed)
    env = ResetCompat(env)
    env = StepCompat(env)
    env = FloatObs(env)
    return DummyVecEnv([lambda: env])

def evaluate(model, venv, n_episodes=10):
    rewards = []
    for _ in range(n_episodes):
        obs = venv.reset()
        done = False
        ep_rew = 0.0
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _info = venv.step(action)
            done = bool(terminated[0] or truncated[0])
            ep_rew += float(reward[0])
        rewards.append(ep_rew)
    return float(np.mean(rewards)), float(np.std(rewards))

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--env-id", type=str, default="HalfCheetah-v4")
    ap.add_argument("--policy-path", type=str, required=True)
    ap.add_argument("--seed", type=int, default=21)
    args = ap.parse_args()

    venv = make_eval_env(args.env_id, args.seed)
    venv = VecNormalize(venv, norm_obs=True, norm_reward=False, training=False)
    model = PPO.load(args.policy_path, env=venv)
    mean_r, std_r = evaluate(model, venv, n_episodes=10)
    print(f"Mean reward: {mean_r:.2f}  |  Std: {std_r:.2f}")
